包括 Keras, Caffe, scikit-learn, libsvm 和 XGBoost

数据分析处理库


init K=[ 2.09816003]
Step #25 K = [ 5.00978231] b = 3.0
Loss = [ 0.48220834]
Step #50 K = [ 4.73436785] b = 3.0
Loss = [ 9.21699047]
Step #75 K = [ 4.8331151] b = 3.0
Loss = [ 14.49190712]
Step #100 K = [ 5.07136011] b = 3.0
Loss = [ 4.86546564]
Step #125 K = [ 5.06736088] b = 3.0
Loss = [ 2.48200941]
tmp loss ok, step=125 k=5.0739 loss=0.000040

Last K = [ 5.0738945]  Loss = [ 54.60419846]

数据集：
训练集
测试集
验证集

2变量线性回归 J(θ0, θ1)

多元线性回归

向量外积，又叫叉积或向量积

特征缩放:
在我们面对多维特征问题的时候，我们要保证这些特征都具有相近的尺度，这将帮助梯度下降算法更快地收敛。

[-1 1]
方法：1)除以最大值 2)均值归一化(mean normalization)   xn = (xn - u) / s   u:平均值  s:最大值-最小值或者标准差

多项式回归 4.5 平方 立方 或者 开方

选择好的特征很重要。
多项式回归模型在使用梯度下降算法前，特征缩放更是重要，否则指数级增长的样本范围使得收敛极慢。


也可以用标准方程法代替梯度下降，用偏导数求出各个θ（特征数量小于1万可接受） ，但使用局限性比较大(需要特征值小, 矩阵可逆(特征不独立、特征数量大于训练集数量可导致不可逆)，大多数算法不适用如分类算法，只能用在回归？)
θ = (X转置 * X) 求逆 * X转置 * y
4.7 正规方程及不可逆性(选修) pdf中有为啥θ等于这个公式的推导过程。

5.6 向量化 要习惯用向量运算代替for循环

6.1 分类问题
逻辑回归是一种分类算法，逻辑回归只是算法的名字。

6.6 除了梯度下降算法还有其他几种算法可以使损失函数最小，如共轭梯度(Conjugate Gradient)、局部优化法(BFGS)、有限内存局部优化法(LBFGS)，不需要选学习率α，收敛更快

6.7 多分类逻辑回归

8.1 非线性假设

8.5 - 8.6 用bool值讲神经网络的原理

10.3 over






